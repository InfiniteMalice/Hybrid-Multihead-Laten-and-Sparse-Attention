{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Dashboard\n",
        "Use this notebook to explore evaluation outputs generated under `results/`.\nUpdate `RESULT_ROOT` to point at your experiment directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n\n",
        "import pandas as pd\n",
        "import plotly.express as px\n\n",
        "RESULT_ROOT = Path('results')\n",
        "CORE_TASKS = ['mmlu', 'gsm8k', 'hellaswag', 'arc_easy', 'arc_challenge', 'boolq']\n",
        "LM_DATASETS = ['wikitext', 'pg19']\n",
        "LONG_SUITES = ['long', 'scrolls']\n",
        "LRA_TASKS = ['listops', 'text', 'retrieval']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_metrics(path: Path) -> Dict:\n",
        "    if not path.exists():\n",
        "        return {}\n",
        "    with path.open() as handle:\n",
        "        return json.load(handle)\n\n",
        "def collect_suite_metrics(model_tag: str, suite: str) -> Dict:\n",
        "    return load_metrics(RESULT_ROOT / model_tag / suite / 'metrics.json')\n\n",
        "def available_models() -> List[str]:\n",
        "    return [p.name for p in RESULT_ROOT.iterdir() if p.is_dir()]\n",
        "\nMODELS = available_models()\n",
        "MODELS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quality_rows = []\n",
        "for model_tag in MODELS:\n",
        "    core = collect_suite_metrics(model_tag, 'core')\n",
        "    for task in CORE_TASKS:\n",
        "        score = core.get('results', {}).get(task, {}).get('accuracy')\n",
        "        if score is not None:\n",
        "            quality_rows.append({'model': model_tag, 'task': task, 'score': score})\n",
        "quality_df = pd.DataFrame(quality_rows)\n",
        "quality_df\n",
        "if not quality_df.empty:\n",
        "    fig = px.bar(quality_df, x='task', y='score', color='model', barmode='group', title='Core benchmark accuracy')\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perf_path = RESULT_ROOT / MODELS[0] / 'perf' / 'latency_mem.csv' if MODELS else None\n",
        "if perf_path and perf_path.exists():\n",
        "    perf_df = pd.read_csv(perf_path)\n",
        "    display(perf_df)\n",
        "    fig = px.line(perf_df, x='seq_len', y='latency_ms', color='batch_size', markers=True, title='Latency vs sequence length')\n",
        "    fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}