"""Multi-head latent attention primitives inspired by DeepSeek-V3 MLA."""
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import torch
from torch import nn
from torch.nn import functional as F

from .sparse_utils import apply_sparse_mask


@dataclass
class AttentionStats:
    """Book-keeping for interpretability hooks and GEPA tracing."""

    head_entropy: Optional[torch.Tensor] = None
    sparsity: Optional[torch.Tensor] = None


class LatentAttention(nn.Module):
    """Latent projection variant of multi-head attention.

    This module mirrors the MLA design described in the DeepSeek-V3 paper.
    The key idea is to project query and key representations to a low-rank
    latent subspace prior to computing attention scores, thereby reducing the
    quadratic complexity in the feature dimension from :math:`O(D^2)` to
    :math:`O(D \cdot d_\text{latent})`.

    Args:
        embed_dim: Size of the model embedding dimension.
        num_heads: Number of attention heads.
        latent_dim_ratio: Ratio between per-head feature dimension and latent
            projection dimension. Must be in ``(0, 1]``.
        dropout: Dropout probability for attention weights.
        use_bias: Whether to include bias terms in the input projections.
        track_stats: If ``True`` the module will compute interpretable metrics
            such as per-head entropy useful for self-evaluation pipelines.
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        latent_dim_ratio: float = 0.25,
        dropout: float = 0.0,
        use_bias: bool = True,
        track_stats: bool = False,
    ) -> None:
        super().__init__()
        if embed_dim % num_heads != 0:
            raise ValueError("embed_dim must be divisible by num_heads")
        if not 0 < latent_dim_ratio <= 1:
            raise ValueError("latent_dim_ratio must be in (0, 1]")

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.latent_dim = max(1, int(self.head_dim * latent_dim_ratio))
        self.scale = self.latent_dim ** -0.5
        self.dropout = dropout
        self.track_stats = track_stats

        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)
        self.q_latent = nn.Linear(self.head_dim, self.latent_dim, bias=False)
        self.k_latent = nn.Linear(self.head_dim, self.latent_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)

        self.attn_drop = nn.Dropout(dropout)
        self.register_buffer("_latest_entropy", torch.tensor(float("nan")), persistent=False)
        self.register_buffer("_latest_sparsity", torch.tensor(float("nan")), persistent=False)

    def _shape_projection(self, x: torch.Tensor) -> torch.Tensor:
        batch, seq_len, _ = x.shape
        x = x.view(batch, seq_len, self.num_heads, self.head_dim)
        return x.permute(0, 2, 1, 3)  # (B, H, T, D_h)

    def _apply_stats(self, attn_weights: torch.Tensor) -> AttentionStats:
        if not self.track_stats:
            return AttentionStats()
        probs = attn_weights.clamp_min(1e-12)
        entropy = -(probs * probs.log()).sum(dim=-1).mean(dim=-1)  # (B, H)
        sparsity = (attn_weights <= 0).float().mean(dim=-1)
        self._latest_entropy = entropy.detach().mean()
        self._latest_sparsity = sparsity.detach().mean()
        return AttentionStats(head_entropy=entropy, sparsity=sparsity)

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        sparse_config: Optional[Dict[str, torch.Tensor]] = None,
        need_weights: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], AttentionStats]:
        """Compute latent attention.

        Args:
            query, key, value: Input tensors with shape ``(B, T, embed_dim)``.
            attention_mask: Optional additive attention mask where negative
                values correspond to masked positions.
            sparse_config: Optional sparse metadata generated by
                :mod:`.sparse_utils`. When provided, masks are applied before
                the softmax.
            need_weights: Return attention map averaged over heads when ``True``.
        """

        q = self._shape_projection(self.q_proj(query))
        k = self._shape_projection(self.k_proj(key))
        v = self._shape_projection(self.v_proj(value))

        q_latent = self.q_latent(q)
        k_latent = self.k_latent(k)

        attn_scores = torch.matmul(q_latent, k_latent.transpose(-2, -1)) * self.scale

        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask

        if sparse_config is not None:
            attn_scores = apply_sparse_mask(attn_scores, sparse_config)

        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_drop(attn_weights)

        stats = self._apply_stats(attn_weights)

        latent_output = torch.matmul(attn_weights, v)
        latent_output = latent_output.permute(0, 2, 1, 3).contiguous()
        latent_output = latent_output.view(query.size(0), query.size(1), self.embed_dim)

        output = self.out_proj(latent_output)

        if need_weights:
            weights = attn_weights.mean(dim=1)
        else:
            weights = None
        return output, weights, stats


class LatentSparseAttention(LatentAttention):
    """Hybrid latent attention with block or top-k sparsity."""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        latent_dim_ratio: float = 0.25,
        dropout: float = 0.0,
        use_bias: bool = True,
        track_stats: bool = False,
        default_sparse_cfg: Optional[Dict[str, torch.Tensor]] = None,
    ) -> None:
        super().__init__(
            embed_dim=embed_dim,
            num_heads=num_heads,
            latent_dim_ratio=latent_dim_ratio,
            dropout=dropout,
            use_bias=use_bias,
            track_stats=track_stats,
        )
        self.default_sparse_cfg = default_sparse_cfg

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        sparse_config: Optional[Dict[str, torch.Tensor]] = None,
        need_weights: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], AttentionStats]:
        sparse = sparse_config or self.default_sparse_cfg
        return super().forward(
            query=query,
            key=key,
            value=value,
            attention_mask=attention_mask,
            sparse_config=sparse,
            need_weights=need_weights,
        )


__all__ = ["LatentAttention", "LatentSparseAttention", "AttentionStats"]
